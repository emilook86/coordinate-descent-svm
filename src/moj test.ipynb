{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ffbc28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import issparse, csc_matrix\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "def load_svm_file(file_path, zero_based=True):\n",
    "    labels = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            parts = line.strip().split()\n",
    "            labels.append(float(parts[0]))\n",
    "\n",
    "            for feat in parts[1:]:\n",
    "                idx, val = feat.split(':')\n",
    "                idx = int(idx) - (0 if zero_based else 1)\n",
    "                rows.append(i)\n",
    "                cols.append(idx)\n",
    "                data.append(float(val))\n",
    "\n",
    "    # Jawna konwersja do CSC\n",
    "    from scipy.sparse import coo_matrix\n",
    "    X = coo_matrix((data, (rows, cols))).tocsc()\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y \n",
    "X,y =load_svm_file('../data/paper_data/news20.binary')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dad047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68d27529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  time = 2025-06-11 23:15:39.440477\n",
      "Inner iteration 9999, error = 0.369217. Time elapsed: 20.40 s.\n",
      "Accuracy on test set at iter 10000: 0.5827\n",
      "Inner iteration 19999, error = 0.272818. Time elapsed: 36.94 s.\n",
      "Accuracy on test set at iter 20000: 0.6887\n",
      "Inner iteration 29999, error = 0.222618. Time elapsed: 54.75 s.\n",
      "Accuracy on test set at iter 30000: 0.7282\n",
      "Inner iteration 39999, error = 0.206927. Time elapsed: 72.97 s.\n",
      "Accuracy on test set at iter 40000: 0.7440\n",
      "Inner iteration 49999, error = 0.173418. Time elapsed: 90.66 s.\n",
      "Accuracy on test set at iter 50000: 0.7768\n",
      "Inner iteration 59999, error = 0.147099. Time elapsed: 108.44 s.\n",
      "Accuracy on test set at iter 60000: 0.8020\n",
      "Inner iteration 69999, error = 0.128345. Time elapsed: 126.21 s.\n",
      "Accuracy on test set at iter 70000: 0.8225\n",
      "Inner iteration 79999, error = 0.119717. Time elapsed: 143.47 s.\n",
      "Accuracy on test set at iter 80000: 0.8300\n",
      "Inner iteration 89999, error = 0.107964. Time elapsed: 162.62 s.\n",
      "Accuracy on test set at iter 90000: 0.8470\n",
      "Inner iteration 99999, error = 0.100463. Time elapsed: 181.01 s.\n",
      "Accuracy on test set at iter 100000: 0.8540\n",
      "Inner iteration 109999, error = 0.090773. Time elapsed: 199.82 s.\n",
      "Accuracy on test set at iter 110000: 0.8668\n",
      "Inner iteration 119999, error = 0.085271. Time elapsed: 218.62 s.\n",
      "Accuracy on test set at iter 120000: 0.8728\n",
      "Inner iteration 129999, error = 0.080208. Time elapsed: 236.81 s.\n",
      "Accuracy on test set at iter 130000: 0.8802\n",
      "Inner iteration 139999, error = 0.074144. Time elapsed: 252.99 s.\n",
      "Accuracy on test set at iter 140000: 0.8848\n",
      "Inner iteration 149999, error = 0.070455. Time elapsed: 269.25 s.\n",
      "Accuracy on test set at iter 150000: 0.8918\n",
      "Inner iteration 159999, error = 0.065516. Time elapsed: 279.90 s.\n",
      "Accuracy on test set at iter 160000: 0.8925\n",
      "Inner iteration 169999, error = 0.060515. Time elapsed: 291.25 s.\n",
      "Accuracy on test set at iter 170000: 0.8972\n",
      "Inner iteration 179999, error = 0.057014. Time elapsed: 302.51 s.\n",
      "Accuracy on test set at iter 180000: 0.8998\n",
      "Inner iteration 189999, error = 0.052638. Time elapsed: 313.96 s.\n",
      "Accuracy on test set at iter 190000: 0.9022\n",
      "Inner iteration 199999, error = 0.049700. Time elapsed: 326.91 s.\n",
      "Accuracy on test set at iter 200000: 0.9073\n",
      "Inner iteration 209999, error = 0.047887. Time elapsed: 341.22 s.\n",
      "Accuracy on test set at iter 210000: 0.9100\n",
      "Inner iteration 219999, error = 0.044949. Time elapsed: 353.33 s.\n",
      "Accuracy on test set at iter 220000: 0.9115\n",
      "Inner iteration 229999, error = 0.043136. Time elapsed: 363.28 s.\n",
      "Accuracy on test set at iter 230000: 0.9125\n",
      "Inner iteration 239999, error = 0.042136. Time elapsed: 374.95 s.\n",
      "Accuracy on test set at iter 240000: 0.9140\n",
      "Inner iteration 249999, error = 0.039447. Time elapsed: 388.39 s.\n",
      "Accuracy on test set at iter 250000: 0.9143\n",
      "Accuracy: 0.91725\n",
      "[(20.396568059921265, np.float64(14910.178973614278)), (36.94266748428345, np.float64(14048.785633940684)), (54.74902391433716, np.float64(13400.241742416863)), (72.96551942825317, np.float64(12747.749580723961)), (90.66293025016785, np.float64(12031.252419802675)), (108.44069600105286, np.float64(11537.643237714205)), (126.21352696418762, np.float64(11134.375702519437)), (143.47077631950378, np.float64(10739.462527145964)), (162.62413048744202, np.float64(10285.542430969384)), (181.01427054405212, np.float64(10094.367119679839)), (199.82416009902954, np.float64(9820.804304074472)), (218.62240529060364, np.float64(9625.220865394744)), (236.8125603199005, np.float64(9406.318610266258)), (252.98625779151917, np.float64(9258.196610097562)), (269.2466711997986, np.float64(9121.25176449458)), (279.89971709251404, np.float64(8991.433135832576)), (291.2536883354187, np.float64(8785.624657507806)), (302.50903606414795, np.float64(8647.010837221442)), (313.955411195755, np.float64(8503.442866271853)), (326.90599942207336, np.float64(8389.130691612976)), (341.2244338989258, np.float64(8307.59879184817)), (353.32939076423645, np.float64(8201.459102062996)), (363.2774410247803, np.float64(8106.494549745712)), (374.9482002258301, np.float64(7995.916172304478)), (388.3866662979126, np.float64(7922.604667819039))]\n",
      "[ 1.         1.        -0.2593738 ...  1.         1.         1.       ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import issparse, csc_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "from data_loader import load_svm_file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SparseCoordinateDescentSVM_2:\n",
    "    def __init__(self, C=1.0, max_iter=10000, tol=1e-8, sigma=0.01, beta=0.5, verbose=True, maxtime = np.inf):\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.sigma = sigma\n",
    "        self.beta = beta\n",
    "        self.verbose = verbose\n",
    "        self.w = None\n",
    "        self.z = None\n",
    "        self.H = None\n",
    "        self.lambdas = {}\n",
    "        self.times = []\n",
    "        self.relative_diffs = []\n",
    "        self.obj_values =[]\n",
    "        self.time_start = time.time()\n",
    "        self.max_time = maxtime\n",
    "\n",
    "\n",
    "        self.objective_values = []\n",
    "        self.gradient_values = []\n",
    "        self.gradient_norm_values = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def _precompute_H(self, X):\n",
    "        \"\"\"\n",
    "            Precompute diagonal elements of Hessian matrix:\n",
    "            H_i = 1 + 2C * sum_j x_ji^2\n",
    "            Used for second derivatives during coordinate updates.\n",
    "        \"\"\"\n",
    "        return  1 + 2 * self.C * (X.power(2).sum(axis=0)).A1\n",
    "    \n",
    "    def _d_prime_i_0(self, X, y, i):\n",
    "        margins = 1 - y * self.z\n",
    "        active = margins > 0\n",
    "\n",
    "        # Indeksy kolumny i w formacie CSC\n",
    "        col_start = X.indptr[i]\n",
    "        col_end = X.indptr[i + 1]\n",
    "        row_indices = X.indices[col_start:col_end]\n",
    "        data = X.data[col_start:col_end]\n",
    "\n",
    "        # Maska aktywnych przykładów w tych wierszach\n",
    "        mask = active[row_indices]\n",
    "\n",
    "        if not np.any(mask):\n",
    "            return self.w[i]  # brak aktywnych przykładów, gradient to tylko w[i]\n",
    "\n",
    "        filtered_y = y[row_indices][mask]\n",
    "        filtered_margins = margins[row_indices][mask]\n",
    "        filtered_data = data[mask]\n",
    "\n",
    "        gradient_sum = np.sum(filtered_data * filtered_y * filtered_margins)\n",
    "\n",
    "        d_prime_i = self.w[i] - 2 * self.C * gradient_sum\n",
    "        self.gradient_values[i] = d_prime_i\n",
    "\n",
    "        return d_prime_i\n",
    "\n",
    "\n",
    "    def _d_double_prime_i_0(self, X, y, i):\n",
    "        # Wyciągamy niezerowe elementy kolumny i z csc_matrix X\n",
    "        col_start = X.indptr[i]\n",
    "        col_end = X.indptr[i + 1]\n",
    "        indices = X.indices[col_start:col_end]  # wiersze niezerowych elementów kolumny i\n",
    "        data = X.data[col_start:col_end]       # wartości tych elementów\n",
    "\n",
    "        # Obliczamy marginesy\n",
    "        margins = 1 - y * self.z\n",
    "        # Tworzymy maskę aktywnych przykładów (tych, które łamią warunek margin > 0)\n",
    "        active_mask = margins[indices] > 0\n",
    "\n",
    "        if not np.any(active_mask):\n",
    "            return 1.0  # tylko regularizacja, brak strat\n",
    "\n",
    "        data_active = data[active_mask]\n",
    "\n",
    "\n",
    "        return 1.0 + 2 * self.C * np.sum(data_active ** 2)\n",
    "\n",
    "\n",
    "    def _newton_direction(self, X, y, i,  denominator = None):\n",
    "\n",
    "        numerator = self._d_prime_i_0(X, y, i)\n",
    "\n",
    "        if (denominator == None):\n",
    "            denominator = self._d_double_prime_i_0(X, y, i)\n",
    "        return -numerator / denominator if denominator != 0 else 0.0\n",
    "\n",
    "    def _d_i_z(self, X, y, i, z):\n",
    "        x_col = X[:, i]\n",
    "        indices = x_col.indices\n",
    "        delta = z * x_col.data\n",
    "        z_new_part = self.z[indices] + delta\n",
    "    \n",
    "        margins_part = 1 - y[indices] * z_new_part\n",
    "        active = margins_part > 0\n",
    "        loss_term = np.sum(margins_part[active] ** 2)\n",
    "    \n",
    "        w_norm_sq = np.dot(self.w, self.w) + 2 * z * self.w[i] + z**2\n",
    "        return 0.5 * w_norm_sq + self.C * loss_term\n",
    "\n",
    "\n",
    "    def _compute_threshhold_lambda(self, X, y, i):\n",
    "  \n",
    "        dii = self._d_double_prime_i_0(X, y, i)\n",
    "\n",
    "        return dii / (0.5 * self.H[i]+ self.sigma), dii\n",
    "\n",
    "\n",
    "\n",
    "    def _compute_lambda(self, X, y, i, d):\n",
    "        lambda_bar = self.lambdas[i]\n",
    "        if abs(d) < 1e-12:\n",
    "            return 0.0\n",
    "        if 1.0 <= lambda_bar:\n",
    "            return 1.0\n",
    "\n",
    "        # Otherwise, perform line search\n",
    "        D0 = self._d_i_z(X, y, i, 0)\n",
    "        k = 0\n",
    "        while True:\n",
    "            lam = self.beta ** k\n",
    "            z = lam * d\n",
    "            Dz = self._d_i_z(X, y, i, z)\n",
    "            if Dz - D0 <= -self.sigma * (z ** 2):\n",
    "                return lam\n",
    "            k += 1\n",
    "            if k > 20:  # Prevent infinite loop\n",
    "                return lam\n",
    "\n",
    "    def _coordinate_update(self, X, y, i, z, dii=None):\n",
    "        d = self._newton_direction(X, y, i, dii)\n",
    "        if abs(d) < 1e-12:\n",
    "            return\n",
    "\n",
    "        lam = 1.0\n",
    "        if self.lambdas[i] < 1.0:  # Only do line search if needed\n",
    "            lam = self._compute_lambda(X, y, i, d)\n",
    "\n",
    "        delta = lam * d\n",
    "        self.w[i] += delta\n",
    "\n",
    "        # Efficient update of self.z for sparse column\n",
    "        col_start = X.indptr[i]\n",
    "        col_end = X.indptr[i + 1]\n",
    "        indices = X.indices[col_start:col_end]\n",
    "        data = X.data[col_start:col_end]\n",
    "\n",
    "        self.z[indices] += delta * data\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, X_test, y_test):\n",
    "        print(f\"start  time = {datetime.datetime.now()}\")\n",
    "        if not issparse(X) or not isinstance(X, csc_matrix):\n",
    "            raise ValueError(\"X must be a CSC (Compressed Sparse Column) matrix\")\n",
    "        #start with initial w0\n",
    "        \n",
    "        self.w = np.zeros(X.shape[1],dtype=np.float64)\n",
    "        self.H = self._precompute_H(X)\n",
    "        self.z = X.dot(self.w)\n",
    "\n",
    "        self.gradient_values = np.ones(X.shape[1])\n",
    "\n",
    "        start_even_before = time.time()\n",
    "        for iteration in range(self.max_iter):\n",
    "            i = np.random.randint(0, X.shape[1])\n",
    "            if i not in self.lambdas:\n",
    "                self.lambdas[i], dii = self._compute_threshhold_lambda(X, y, i)\n",
    "            else:\n",
    "                dii = None\n",
    "            self._coordinate_update(X, y, i, dii)\n",
    "            \n",
    "            elapsed = time.time() - start_even_before\n",
    "            if iteration % 10000 == 9999:\n",
    "                    f_w = self._objective(y=y)\n",
    "                    self.objective_values.append((elapsed, f_w))\n",
    "                    print(f\"Inner iteration {iteration}, error = {1 - self.score(X, y):.6f}. Time elapsed: {elapsed:.2f} s.\")\n",
    "\n",
    "                    final_grad_norm = np.linalg.norm(self.gradient_values)\n",
    "                    self.gradient_norm_values.append((elapsed, final_grad_norm))\n",
    "                    if X_test is not None and y_test is not None:\n",
    "                        accuracy = self.score(X_test, y_test)\n",
    "                        print(f\"Accuracy on test set at iter {iteration+1}: {accuracy:.4f}\")\n",
    "                        self.accuracies.append((elapsed, accuracy))\n",
    "\n",
    "            if elapsed > self.max_time:\n",
    "                return self\n",
    "\n",
    "            elapsed = time.time() - start_even_before\n",
    "            final_grad_norm = np.linalg.norm(self.gradient_values)\n",
    "            self.gradient_norm_values.append((elapsed, final_grad_norm))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        print(f\"finish  time = {datetime.datetime.now()}\")\n",
    "\n",
    "        print(f\"Exited outer iteration loop number {iteration+1}, grad_norm = {final_grad_norm:.4f}, time: {elapsed:.2f} s.\")\n",
    "        print(f\"And moreover, objective value function: {self.objective_values[-1][1]}\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X @self.w)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "        \n",
    "    def _objective(self, y):\n",
    "        margins = 1 - y * self.z\n",
    "        loss = np.sum((margins[margins > 0]) ** 2)\n",
    "        return 0.5 * np.dot(self.w, self.w) + self.C * loss\n",
    "\n",
    "model = SparseCoordinateDescentSVM_2(C=1.0, max_iter=11355192,maxtime = 400)\n",
    "#X,y = load_svm_file('../data/paper_data/news20.binary')\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "#X_train = X_train.tocsc()\n",
    "\n",
    "model.fit(X_train, y_train, X_test, y_test)\n",
    "score_model1 = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {score_model1}\")\n",
    "print(model.objective_values)\n",
    "print(model.gradient_values)\n",
    "data = 'news20'\n",
    "np.save(f'model4_{data}_objective_values.npy', model.objective_values)\n",
    "np.save(f'model4_{data}_gradient_values.npy', model.gradient_norm_values)\n",
    "np.save(f'model4_{data}_accuracy_values.npy', model.accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a479a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1355192,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f11931d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1355192)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84528f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4000x1355192 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1786585 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7509c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:100] @ model.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a27a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38cda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80171ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118f76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dff00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start  time = 2025-06-12 00:15:45.340815\n",
      "Inner iteration 9999, error = 0.348587. Time elapsed: 12.94 s.\n",
      "Accuracy on test set at iter 10000: 0.6160\n",
      "Inner iteration 19999, error = 0.238497. Time elapsed: 23.20 s.\n",
      "Accuracy on test set at iter 20000: 0.7232\n",
      "Inner iteration 29999, error = 0.192923. Time elapsed: 34.03 s.\n",
      "Accuracy on test set at iter 30000: 0.7682\n",
      "Inner iteration 39999, error = 0.168917. Time elapsed: 45.31 s.\n",
      "Accuracy on test set at iter 40000: 0.7887\n",
      "Inner iteration 49999, error = 0.141035. Time elapsed: 56.67 s.\n",
      "Accuracy on test set at iter 50000: 0.8130\n",
      "Inner iteration 59999, error = 0.124719. Time elapsed: 69.02 s.\n",
      "Accuracy on test set at iter 60000: 0.8315\n",
      "Inner iteration 69999, error = 0.115466. Time elapsed: 81.16 s.\n",
      "Accuracy on test set at iter 70000: 0.8433\n",
      "Inner iteration 79999, error = 0.103713. Time elapsed: 93.41 s.\n",
      "Accuracy on test set at iter 80000: 0.8540\n",
      "Inner iteration 89999, error = 0.092961. Time elapsed: 105.76 s.\n",
      "Accuracy on test set at iter 90000: 0.8652\n",
      "Inner iteration 99999, error = 0.086647. Time elapsed: 117.97 s.\n",
      "Accuracy on test set at iter 100000: 0.8745\n",
      "Inner iteration 109999, error = 0.080770. Time elapsed: 129.73 s.\n",
      "Accuracy on test set at iter 110000: 0.8775\n",
      "Inner iteration 119999, error = 0.076019. Time elapsed: 141.45 s.\n",
      "Accuracy on test set at iter 120000: 0.8828\n",
      "Inner iteration 129999, error = 0.070205. Time elapsed: 153.26 s.\n",
      "Accuracy on test set at iter 130000: 0.8870\n",
      "Inner iteration 139999, error = 0.065641. Time elapsed: 165.84 s.\n",
      "Accuracy on test set at iter 140000: 0.8910\n",
      "Inner iteration 149999, error = 0.061078. Time elapsed: 177.54 s.\n",
      "Accuracy on test set at iter 150000: 0.8968\n",
      "Inner iteration 159999, error = 0.058577. Time elapsed: 189.26 s.\n",
      "Accuracy on test set at iter 160000: 0.8970\n",
      "Inner iteration 169999, error = 0.054701. Time elapsed: 200.31 s.\n",
      "Accuracy on test set at iter 170000: 0.8985\n",
      "Inner iteration 179999, error = 0.050950. Time elapsed: 210.76 s.\n",
      "Accuracy on test set at iter 180000: 0.9008\n",
      "Inner iteration 189999, error = 0.046762. Time elapsed: 221.31 s.\n",
      "Accuracy on test set at iter 190000: 0.9095\n",
      "Inner iteration 199999, error = 0.044199. Time elapsed: 232.17 s.\n",
      "Accuracy on test set at iter 200000: 0.9130\n",
      "Inner iteration 209999, error = 0.042261. Time elapsed: 242.62 s.\n",
      "Accuracy on test set at iter 210000: 0.9147\n",
      "Inner iteration 219999, error = 0.039447. Time elapsed: 255.68 s.\n",
      "Accuracy on test set at iter 220000: 0.9177\n",
      "Inner iteration 229999, error = 0.037259. Time elapsed: 263.23 s.\n",
      "Accuracy on test set at iter 230000: 0.9210\n",
      "Inner iteration 239999, error = 0.036197. Time elapsed: 268.65 s.\n",
      "Accuracy on test set at iter 240000: 0.9210\n",
      "Inner iteration 249999, error = 0.033758. Time elapsed: 273.82 s.\n",
      "Accuracy on test set at iter 250000: 0.9227\n",
      "Inner iteration 259999, error = 0.032633. Time elapsed: 278.10 s.\n",
      "Accuracy on test set at iter 260000: 0.9223\n",
      "Inner iteration 269999, error = 0.030445. Time elapsed: 282.32 s.\n",
      "Accuracy on test set at iter 270000: 0.9240\n",
      "Inner iteration 279999, error = 0.029007. Time elapsed: 286.54 s.\n",
      "Accuracy on test set at iter 280000: 0.9265\n",
      "Inner iteration 289999, error = 0.027694. Time elapsed: 291.39 s.\n",
      "Accuracy on test set at iter 290000: 0.9280\n",
      "Inner iteration 299999, error = 0.026257. Time elapsed: 298.26 s.\n",
      "Accuracy on test set at iter 300000: 0.9305\n",
      "Inner iteration 309999, error = 0.025756. Time elapsed: 305.17 s.\n",
      "Accuracy on test set at iter 310000: 0.9313\n",
      "Inner iteration 319999, error = 0.025194. Time elapsed: 310.89 s.\n",
      "Accuracy on test set at iter 320000: 0.9323\n",
      "Inner iteration 329999, error = 0.023131. Time elapsed: 315.38 s.\n",
      "Accuracy on test set at iter 330000: 0.9330\n",
      "Inner iteration 339999, error = 0.021818. Time elapsed: 319.68 s.\n",
      "Accuracy on test set at iter 340000: 0.9350\n",
      "Inner iteration 349999, error = 0.021255. Time elapsed: 323.96 s.\n",
      "Accuracy on test set at iter 350000: 0.9360\n",
      "Inner iteration 359999, error = 0.020130. Time elapsed: 329.62 s.\n",
      "Accuracy on test set at iter 360000: 0.9350\n",
      "Inner iteration 369999, error = 0.019817. Time elapsed: 335.21 s.\n",
      "Accuracy on test set at iter 370000: 0.9353\n",
      "Inner iteration 379999, error = 0.019442. Time elapsed: 342.07 s.\n",
      "Accuracy on test set at iter 380000: 0.9373\n",
      "Inner iteration 389999, error = 0.018442. Time elapsed: 347.61 s.\n",
      "Accuracy on test set at iter 390000: 0.9380\n",
      "Inner iteration 399999, error = 0.017879. Time elapsed: 352.98 s.\n",
      "Accuracy on test set at iter 400000: 0.9383\n",
      "Inner iteration 409999, error = 0.017317. Time elapsed: 357.23 s.\n",
      "Accuracy on test set at iter 410000: 0.9383\n",
      "Inner iteration 419999, error = 0.016879. Time elapsed: 362.83 s.\n",
      "Accuracy on test set at iter 420000: 0.9393\n",
      "Inner iteration 429999, error = 0.015629. Time elapsed: 368.48 s.\n",
      "Accuracy on test set at iter 430000: 0.9397\n",
      "Inner iteration 439999, error = 0.015254. Time elapsed: 373.51 s.\n",
      "Accuracy on test set at iter 440000: 0.9395\n",
      "Inner iteration 449999, error = 0.014816. Time elapsed: 377.66 s.\n",
      "Accuracy on test set at iter 450000: 0.9410\n",
      "Inner iteration 459999, error = 0.014191. Time elapsed: 381.91 s.\n",
      "Accuracy on test set at iter 460000: 0.9415\n",
      "Inner iteration 469999, error = 0.013753. Time elapsed: 386.39 s.\n",
      "Accuracy on test set at iter 470000: 0.9423\n",
      "Inner iteration 479999, error = 0.013253. Time elapsed: 391.08 s.\n",
      "Accuracy on test set at iter 480000: 0.9435\n",
      "Inner iteration 489999, error = 0.012503. Time elapsed: 395.42 s.\n",
      "Accuracy on test set at iter 490000: 0.9443\n",
      "Accuracy: 0.9445\n",
      "[(12.938154697418213, np.float64(14658.1392460484)), (23.19646167755127, np.float64(13494.749810296698)), (34.02960562705994, np.float64(12886.99495379632)), (45.308504819869995, np.float64(12170.570653888652)), (56.66882514953613, np.float64(11480.378539382076)), (69.02061986923218, np.float64(11014.923562097678)), (81.1614921092987, np.float64(10539.935856327747)), (93.41272950172424, np.float64(10190.197679956345)), (105.76138377189636, np.float64(9862.748914411543)), (117.96870446205139, np.float64(9631.774152702075)), (129.72579216957092, np.float64(9459.824097167937)), (141.4491045475006, np.float64(9242.28476063743)), (153.25535941123962, np.float64(9029.94619042215)), (165.83964920043945, np.float64(8894.721464035792)), (177.54301476478577, np.float64(8750.319139073616)), (189.2619640827179, np.float64(8638.69236184813)), (200.3075978755951, np.float64(8499.986244497686)), (210.75588512420654, np.float64(8387.105453334316)), (221.31268978118896, np.float64(8228.66833917304)), (232.16656804084778, np.float64(8156.0606357313045)), (242.61850762367249, np.float64(8072.554340401681)), (255.67545676231384, np.float64(7992.673170855754)), (263.22661113739014, np.float64(7923.7232469352475)), (268.647882938385, np.float64(7874.427925998429)), (273.81947445869446, np.float64(7798.657029944474)), (278.0957636833191, np.float64(7739.511032552704)), (282.3209385871887, np.float64(7681.703754302058)), (286.5411584377289, np.float64(7633.99943352357)), (291.39445900917053, np.float64(7574.036219134853)), (298.25782895088196, np.float64(7526.910424946157)), (305.1650536060333, np.float64(7488.545383334038)), (310.8910937309265, np.float64(7456.137617488919)), (315.3779253959656, np.float64(7404.182913553158)), (319.6756646633148, np.float64(7363.671081066436)), (323.9620487689972, np.float64(7334.432214599233)), (329.6172502040863, np.float64(7298.007253635207)), (335.2050907611847, np.float64(7268.801659084986)), (342.0672504901886, np.float64(7239.545246567346)), (347.6060748100281, np.float64(7210.451653917267)), (352.98095655441284, np.float64(7184.265194335512)), (357.23401141166687, np.float64(7160.95567472919)), (362.8324773311615, np.float64(7140.414992625819)), (368.4757122993469, np.float64(7116.198441996833)), (373.50684928894043, np.float64(7092.60331047239)), (377.66188383102417, np.float64(7071.992048852567)), (381.91395831108093, np.float64(7049.084775246389)), (386.39238929748535, np.float64(7032.785082391675)), (391.0790593624115, np.float64(7013.69988970062)), (395.41779947280884, np.float64(6995.352522968945))]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import issparse, csc_matrix\n",
    "import datetime\n",
    "\n",
    "\n",
    "class SparseCoordinateDescentSVM_3:\n",
    "    def __init__(self, C=1.0, max_iter=10000, tol=1e-8, sigma=0.01, beta=0.5, verbose=True, max_time = 400):\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.sigma = sigma\n",
    "        self.beta = beta\n",
    "        self.verbose = verbose\n",
    "        self.w = None\n",
    "        self.z = None\n",
    "        self.H = None\n",
    "        self.lambdas = {}\n",
    "        self.times = []\n",
    "        self.relative_diffs = []\n",
    "        self.obj_values =[]\n",
    "        self.time_start = time.time()\n",
    "\n",
    "        self.objective_values = []\n",
    "        self.gradient_values = []\n",
    "        self.gradient_norm_values = []\n",
    "        self.accuracies = []\n",
    "        self.max_time = max_time\n",
    "\n",
    "\n",
    "    def _precompute_H(self, X):\n",
    "        \"\"\"\n",
    "            Precompute diagonal elements of Hessian matrix:\n",
    "            H_i = 1 + 2C * sum_j x_ji^2\n",
    "            Used for second derivatives during coordinate updates.\n",
    "        \"\"\"\n",
    "        return  1 + 2 * self.C * (X.power(2).sum(axis=0)).A1\n",
    "    def _d_prime_i_0(self, X, y, i):\n",
    "        margins = 1 - y * self.z\n",
    "        active = margins > 0\n",
    "\n",
    "        # Indeksy kolumny i w formacie CSC\n",
    "        col_start = X.indptr[i]\n",
    "        col_end = X.indptr[i + 1]\n",
    "        row_indices = X.indices[col_start:col_end]\n",
    "        data = X.data[col_start:col_end]\n",
    "\n",
    "        # Maska aktywnych przykładów w tych wierszach\n",
    "        mask = active[row_indices]\n",
    "\n",
    "        if not np.any(mask):\n",
    "            return self.w[i]  # brak aktywnych przykładów, gradient to tylko w[i]\n",
    "\n",
    "        filtered_y = y[row_indices][mask]\n",
    "        filtered_margins = margins[row_indices][mask]\n",
    "        filtered_data = data[mask]\n",
    "\n",
    "        gradient_sum = np.sum(filtered_data * filtered_y * filtered_margins)\n",
    "        d_prime_i = self.w[i] - 2 * self.C * gradient_sum\n",
    "        self.gradient_values[i] = d_prime_i\n",
    "        return d_prime_i\n",
    "\n",
    "\n",
    "    def _d_double_prime_i_0(self, X, y, i):\n",
    "        # Wyciągamy niezerowe elementy kolumny i z csc_matrix X\n",
    "        col_start = X.indptr[i]\n",
    "        col_end = X.indptr[i + 1]\n",
    "        indices = X.indices[col_start:col_end]  # wiersze niezerowych elementów kolumny i\n",
    "        data = X.data[col_start:col_end]       # wartości tych elementów\n",
    "\n",
    "        # Obliczamy marginesy\n",
    "        margins = 1 - y * self.z\n",
    "        # Tworzymy maskę aktywnych przykładów (tych, które łamią warunek margin > 0)\n",
    "        active_mask = margins[indices] > 0\n",
    "\n",
    "        if not np.any(active_mask):\n",
    "            return 1.0  # tylko regularizacja, brak strat\n",
    "\n",
    "        data_active = data[active_mask]\n",
    "\n",
    "        return 1.0 + 2 * self.C * np.sum(data_active ** 2)\n",
    "\n",
    "\n",
    "    def _newton_direction(self, X, y, i,  denominator = None):\n",
    "\n",
    "        numerator = self._d_prime_i_0(X, y, i)\n",
    "\n",
    "        if (denominator == None):\n",
    "            denominator = self._d_double_prime_i_0(X, y, i)\n",
    "        return -numerator / denominator if denominator != 0 else 0.0\n",
    "\n",
    "    def _d_i_z(self, X, y, i, z):\n",
    "        x_col = X[:, i]\n",
    "        indices = x_col.indices\n",
    "        delta = z * x_col.data\n",
    "        z_new_part = self.z[indices] + delta\n",
    "    \n",
    "        margins_part = 1 - y[indices] * z_new_part\n",
    "        active = margins_part > 0\n",
    "        loss_term = np.sum(margins_part[active] ** 2)\n",
    "    \n",
    "        w_norm_sq = np.dot(self.w, self.w) + 2 * z * self.w[i] + z**2\n",
    "        return 0.5 * w_norm_sq + self.C * loss_term\n",
    "\n",
    "\n",
    "    def _compute_threshhold_lambda(self, X, y, i):\n",
    "  \n",
    "        dii = self._d_double_prime_i_0(X, y, i)\n",
    "\n",
    "        return dii / (0.5 * self.H[i]+ self.sigma), dii\n",
    "\n",
    "\n",
    "\n",
    "    def _compute_lambda(self, X, y, i, d):\n",
    "        lambda_bar = self.lambdas[i]\n",
    "        if abs(d) < 1e-12:\n",
    "            return 0.0\n",
    "        if 1.0 <= lambda_bar:\n",
    "            return 1.0\n",
    "\n",
    "        # Otherwise, perform line search\n",
    "        D0 = self._d_i_z(X, y, i, 0)\n",
    "        k = 0\n",
    "        while True:\n",
    "            lam = self.beta ** k\n",
    "            z = lam * d\n",
    "            Dz = self._d_i_z(X, y, i, z)\n",
    "            if Dz - D0 <= -self.sigma * (z ** 2):\n",
    "                return lam\n",
    "            k += 1\n",
    "            if k > 20:  # Prevent infinite loop\n",
    "                return lam\n",
    "\n",
    "    def _coordinate_update(self, X, y, i, z, dii=None):\n",
    "        d = self._newton_direction(X, y, i, dii)\n",
    "        if abs(d) < 1e-12:\n",
    "            return\n",
    "\n",
    "        lam = 1.0\n",
    "        if self.lambdas[i] < 1.0:  # Only do line search if needed\n",
    "            lam = self._compute_lambda(X, y, i, d)\n",
    "\n",
    "        delta = lam * d\n",
    "        self.w[i] += delta\n",
    "\n",
    "        # Efficient update of self.z for sparse column\n",
    "        col_start = X.indptr[i]\n",
    "        col_end = X.indptr[i + 1]\n",
    "        indices = X.indices[col_start:col_end]\n",
    "        data = X.data[col_start:col_end]\n",
    "\n",
    "        self.z[indices] += delta * data\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, X_test, y_test):\n",
    "        print(f\"start  time = {datetime.datetime.now()}\")\n",
    "        if not issparse(X) or not isinstance(X, csc_matrix):\n",
    "            raise ValueError(\"X must be a CSC (Compressed Sparse Column) matrix\")\n",
    "        #start with initial w0\n",
    "        \n",
    "        self.w = np.zeros(X.shape[1],dtype=np.float64)\n",
    "        self.H = self._precompute_H(X)\n",
    "        self.z = X.dot(self.w)\n",
    "        self.gradient_values = np.ones(X.shape[1])\n",
    "\n",
    "        start_even_before =time.time()\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            perm = np.random.permutation(X.shape[1])\n",
    "            counter = 0\n",
    "            for i in perm:\n",
    "                counter +=1\n",
    "                if i not in self.lambdas:\n",
    "                    self.lambdas[i], dii = self._compute_threshhold_lambda(X, y, i)\n",
    "                else:\n",
    "                    dii = None\n",
    "                self._coordinate_update(X, y, i, dii)\n",
    "                \n",
    "                elapsed = time.time() - start_even_before\n",
    "                if counter % 10000 == 9999:\n",
    "                        f_w = self._objective(y=y)\n",
    "                        self.objective_values.append((elapsed, f_w))\n",
    "                        print(f\"Inner iteration {counter}, error = {1 - self.score(X, y):.6f}. Time elapsed: {elapsed:.2f} s.\")\n",
    "\n",
    "                        final_grad_norm = np.linalg.norm(self.gradient_values)\n",
    "                        self.gradient_norm_values.append((elapsed, final_grad_norm))\n",
    "                        if X_test is not None and y_test is not None:\n",
    "                            accuracy = self.score(X_test, y_test)\n",
    "                            print(f\"Accuracy on test set at iter {counter+1}: {accuracy:.4f}\")\n",
    "                            self.accuracies.append((elapsed, accuracy))\n",
    "\n",
    "                if elapsed > self.max_time:\n",
    "                    return self\n",
    "\n",
    "                elapsed = time.time() - start_even_before\n",
    "                final_grad_norm = np.linalg.norm(self.gradient_values)\n",
    "                self.gradient_norm_values.append((elapsed, final_grad_norm))\n",
    "    \n",
    "\n",
    "        print(f\"finish  time = {datetime.datetime.now()}\")\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(X @self.w)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "        \n",
    "    def _objective(self, y):\n",
    "        margins = 1 - y * self.z\n",
    "        loss = np.sum((margins[margins > 0]) ** 2)\n",
    "        return 0.5 * np.dot(self.w, self.w) + self.C * loss\n",
    "\n",
    "model = SparseCoordinateDescentSVM_3(C=1.0, max_iter=1\n",
    "                                     )\n",
    "\n",
    "\n",
    "#X,y = load_svm_file('../data/paper_data/news20.binary')\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n",
    "#X_train = X_train.tocsc()\n",
    "\n",
    "model.fit(X_train, y_train, X_test, y_test)\n",
    "score_model1 = model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {score_model1}\")\n",
    "print(model.objective_values)\n",
    "print(model.gradient_values)\n",
    "data = 'news20'\n",
    "np.save(f'model5_{data}_objective_values.npy', model.objective_values)\n",
    "np.save(f'model5_{data}_gradient_values.npy', model.gradient_norm_values)\n",
    "np.save(f'model5_{data}_accuracy_values.npy', model.accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c607bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
